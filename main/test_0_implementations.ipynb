{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8959feca",
   "metadata": {},
   "source": [
    "# Testing PINN implementations\n",
    "\n",
    "- Autograd vs. PyTorch\n",
    "- Gradient descent methods (optimization methods)\n",
    "- Learning rates (epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed2d2e809e40f8aaa6baa02025edfa",
   "metadata": {},
   "source": [
    "## Setup and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setup: add repo root to path and import project helpers.\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e33de903e842fe8d73e9d3e7e59fb4",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b709e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# CONFIGURATION INFORMATION : \n",
    "##################################################################################\n",
    "# NETWORK ARCITECTURE : \n",
    "num_hidden_neurons = [2,2]  # A simple network just for testing\n",
    "opt_met            = ['GD', 'GD-adam', 'SGD-adam']\n",
    "# HYPERPARAMETERS : \n",
    "epochs         = 100  \n",
    "learning_rate  = [0.1, 0.01, 0.001, 0.0001]\n",
    "seed           = 36  # Most similar seed for numpy.random.seed and PyTorch.manual_seed\n",
    "# ONLY FOR SGD : \n",
    "batch_fraction = 0.1\n",
    "# GRID RESOLUTION : \n",
    "Nx = 10    # Smaller grid resolution for testing\n",
    "Nt = 10    # Smaller grid resolution for testing\n",
    "\n",
    "batch_size = 10\n",
    "# MAKE X AND T : \n",
    "t, x, t_torch, x_torch, _ = create_t_and_x_batch_size(Nx,\n",
    "                                                      Nt,\n",
    "                                                      batch_fraction)\n",
    "# CONFIGURATION INFORMATION : \n",
    "config = make_config(Nx,\n",
    "                     Nt, \n",
    "                     num_hidden_neurons,\n",
    "                     opt_met, \n",
    "                     epochs, \n",
    "                     learning_rate, \n",
    "                     batch_fraction,\n",
    "                     batch_size,\n",
    "                     seed,\n",
    "                     verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f1d200670465b9072e006517f3bee",
   "metadata": {},
   "source": [
    "## Training helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f79c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# FUNCTIONS FOR TESTING GRADIENT DECENT METHODS AND LEARNING RATES : \n",
    "##################################################################################\n",
    "def compare_cost_and_time_A(optimization_method,learning_rate):\n",
    "    start_time = time.time()\n",
    "    P, history = train_PINN_autograd(x,\n",
    "                                     t,\n",
    "                                     num_hidden_neurons,\n",
    "                                     epochs,\n",
    "                                     learning_rate,\n",
    "                                     'tanh',\n",
    "                                     seed,\n",
    "                                     optimization_method=optimization_method, # 'GD', 'GD-adam', 'SGD-adam'\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     replacement=False,\n",
    "                                     verbose=False,\n",
    "                                     debug=False)\n",
    "    end_time = time.time()\n",
    "    tot_time = end_time - start_time\n",
    "    return tot_time, history, P\n",
    "def compare_cost_and_time_P(optimization_method,learning_rate):\n",
    "    start_time = time.time()\n",
    "    P, history = train_PINN_pyTorch(x_torch,\n",
    "                                    t_torch,\n",
    "                                    num_hidden_neurons,\n",
    "                                    epochs,\n",
    "                                    learning_rate,\n",
    "                                    'tanh',\n",
    "                                    seed,\n",
    "                                    optimization_method=optimization_method, # 'GD', 'GD-adam', 'SGD-adam'\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    replacement=False,\n",
    "                                    verbose=False,\n",
    "                                    debug=False)\n",
    "    end_time = time.time()\n",
    "    tot_time = end_time - start_time\n",
    "    return tot_time, history, P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9398fc1184175acc6122fe0116f17",
   "metadata": {},
   "source": [
    "## Run training sweeps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# TESTING PINN-IMPLEMENTATINOS : \n",
    "##################################################################################\n",
    "learning_rates = learning_rate\n",
    "# RUN TEST : \n",
    "results_A = {}\n",
    "results_P = {}\n",
    "for lr in learning_rates:\n",
    "    print('----------------------------------')\n",
    "    print(f'Learning rate: {lr}')\n",
    "    print('----------------------------------')\n",
    "    res_A = {}\n",
    "    res_P = {}\n",
    "    for opt in opt_met:\n",
    "        print(f'Optimization method: {opt}')\n",
    "        # Run the comparison for autograd and pytorch\n",
    "        print('Autograd')\n",
    "        time_A, hist_A, PA = compare_cost_and_time_A(opt, lr)\n",
    "        print('PyTorch')\n",
    "        time_P, hist_P, PP = compare_cost_and_time_P(opt, lr)\n",
    "        # Save the results in dict, to plot later\n",
    "        res_A[opt] = {'time': time_A, 'history': hist_A, 'P': PA}\n",
    "        res_P[opt] = {'time': time_P, 'history': hist_P, 'P': PP}\n",
    "        print()\n",
    "    results_A[lr] = res_A\n",
    "    results_P[lr] = res_P\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a386954e8db4661afe799f594ef8d20",
   "metadata": {},
   "source": [
    "## Plot: final cost vs learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90af687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cost comparison plot between autograd and pytorch, for optimization methods and learning rates\n",
    "# Learning rates on x-axis, final cost on y-axis\n",
    "# NB! ONLY PLOTTING THE EXECUTION TIME FOR THE FIRST LEARNING RATE\n",
    "plt.figure(figsize=(10,6))\n",
    "for opt in opt_met:\n",
    "    final_costs_A = [results_A[lr][opt]['history']['cost'][-1] for lr in learning_rates]\n",
    "    final_costs_P = [results_P[lr][opt]['history']['cost'][-1] for lr in learning_rates]\n",
    "    time_A = [results_A[lr][opt]['time'] for lr in learning_rates]\n",
    "    time_P = [results_P[lr][opt]['time'] for lr in learning_rates]\n",
    "    plt.plot(learning_rates, final_costs_A, marker='o', label=f'{opt} Time: {time_A[0]:.2f}s', linestyle='--')\n",
    "    plt.plot(learning_rates, final_costs_P, marker='o', label=f'{opt} Time: {time_P[0]:.2f}s', linestyle='-')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Cost')\n",
    "# log scale for x-axis\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title(f'Final Cost Comparison of learning rates, with epoch={epochs}')\n",
    "save_fig('test0_final_cost_vs_lr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c655779fc946e194d2adaf6d1f2b8b",
   "metadata": {},
   "source": [
    "## Plot: cost history comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8da5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the results, comparing autograd and pytorch, for different optimization methods\n",
    "plt.figure(figsize=(10,6))\n",
    "for learning_rate in learning_rates:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    lr_slug = str(learning_rate).replace('.', 'p')\n",
    "    for opt in opt_met: \n",
    "        plt.plot(results_A[learning_rate][opt]['history']['epoch'], results_A[learning_rate][opt]['history']['cost'], label=f'{opt} Time: {results_A[learning_rate][opt][\"time\"]:.2f}s', linestyle='--')\n",
    "        plt.plot(results_P[learning_rate][opt]['history']['epoch'], results_P[learning_rate][opt]['history']['cost'], label=f'{opt} Time: {results_P[learning_rate][opt][\"time\"]:.2f}s', linestyle='-')\n",
    "    #plt.yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "    plt.title('Comparison of Autograd and PyTorch PINN Training {learning_rate='+str(learning_rate)+'}')\n",
    "    save_fig(f'test0_cost_history_lr_{lr_slug}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
