{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN vs FTCS (part b) - timing and accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03cd4ef",
   "metadata": {},
   "source": [
    "## Setup and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229971fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the notebook can import from src/ when run inside code/main_clean\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src import *\n",
    "from time import perf_counter\n",
    "\n",
    "# Placeholder for results so later cells can run safely before the loop\n",
    "test_results = []\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da246aec1e4cfc8fd6f8cedab235aa",
   "metadata": {},
   "source": [
    "## Training configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed best architecture from test2\n",
    "architecture = [40, 40]\n",
    "activation = 'sigmoid'\n",
    "activation_label = activation\n",
    "\n",
    "# Training hyperparameters (keep modest for notebook speed)\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "seed = 36\n",
    "\n",
    "# Training grid\n",
    "torch.manual_seed(seed)\n",
    "Nx_train = 30\n",
    "Nt_train = 30\n",
    "x_torch_train = torch.linspace(0.0, 1.0, Nx_train)\n",
    "t_torch_train = torch.linspace(0.0, 1.0, Nt_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec7a3a4cd0440595447dcb062c4eaf",
   "metadata": {},
   "source": [
    "## Train PINN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc71954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PINN and time it\n",
    "train_start = perf_counter()\n",
    "P_best, history = train_PINN_pyTorch(x_torch_train,\n",
    "                                    t_torch_train,\n",
    "                                    architecture,\n",
    "                                    epochs,\n",
    "                                    learning_rate,\n",
    "                                    activation,\n",
    "                                    seed,\n",
    "                                    optimization_method='SGD-adam',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    replacement=False,\n",
    "                                    verbose=False,\n",
    "                                    debug=False)\n",
    "train_time = perf_counter() - train_start\n",
    "train_final_cost = float(history['cost'][-1]) if history and 'cost' in history else float('nan')\n",
    "print(f\"Training done: {train_time:.3f} s | final cost {train_final_cost:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf1139dc4a4173bea3ba5530089910",
   "metadata": {},
   "source": [
    "## Evaluation helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to evaluate a trained PINN on a grid.\n",
    "def evaluate_pinn_on_grid(P, x_torch, t_torch, architecture, activation):\n",
    "    \"\"\"Run PINN forward pass on a grid, return outputs, max abs error, cost, and runtime.\"\"\"\n",
    "    model = Net(network=architecture, activation=activation).to(device)\n",
    "    model.load_state_dict(P)\n",
    "    model.eval()\n",
    "    Nx = len(x_torch)\n",
    "    Nt = len(t_torch)\n",
    "    with torch.no_grad():\n",
    "        start = perf_counter()\n",
    "        g_dnn = np.zeros((Nx, Nt))\n",
    "        G_an = np.zeros((Nx, Nt))\n",
    "        for i, x_ in enumerate(x_torch):\n",
    "            for j, t_ in enumerate(t_torch):\n",
    "                point = np.array([x_, t_])\n",
    "                g_dnn[i, j] = trial_function_pyTorch(x_, t_, model).detach().cpu().numpy()\n",
    "                G_an[i, j] = g_analytic(point)\n",
    "        infer_time = perf_counter() - start\n",
    "    diff = g_dnn - G_an\n",
    "    max_abs = float(np.max(np.abs(diff)))\n",
    "    # Cost (requires grad, so rebuild model with grad tracking)\n",
    "    model_train = Net(network=architecture, activation=activation).to(device)\n",
    "    model_train.load_state_dict(P)\n",
    "    cost_val = float(cost_function_pyTorch(x_torch, t_torch, model_train).item())\n",
    "    return {\n",
    "        'g_dnn': g_dnn,\n",
    "        'G_an': G_an,\n",
    "        'max_abs': max_abs,\n",
    "        'cost': cost_val,\n",
    "        'infer_time': infer_time,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b6c5928a241db86f824b194417711",
   "metadata": {},
   "source": [
    "## Grid sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test grids (matching test_3 style)\n",
    "Nx_list = [10, 30, 60, 90, 120]\n",
    "Nt_list = [10, 30, 60, 90, 120]\n",
    "target_grid = (30, 30)\n",
    "plot_data = None\n",
    "\n",
    "# Ensure target grid is included\n",
    "if target_grid[0] not in Nx_list:\n",
    "    Nx_list.append(target_grid[0])\n",
    "if target_grid[1] not in Nt_list:\n",
    "    Nt_list.append(target_grid[1])\n",
    "\n",
    "# Keep paired by index; sort by Nx to keep plot ordering stable\n",
    "pairs = sorted(zip(Nx_list, Nt_list), key=lambda p: p[0])\n",
    "\n",
    "for idx,(Nx, Nt) in enumerate(pairs):\n",
    "    x_t = torch.linspace(0.0, 1.0, Nx)\n",
    "    t_t = torch.linspace(0.0, 1.0, Nt)\n",
    "    x_np = x_t.detach().cpu().numpy()\n",
    "    t_np = t_t.detach().cpu().numpy()\n",
    "    # PINN inference\n",
    "    pinn_res = evaluate_pinn_on_grid(P_best, x_t, t_t, architecture, activation)\n",
    "    # FTCS baseline\n",
    "    ftcs_start = perf_counter()\n",
    "    ftcs_grid = ftcs_solution(x_np, t_np)\n",
    "    ftcs_time = perf_counter() - ftcs_start\n",
    "    G_an_ftcs = np.zeros_like(ftcs_grid)\n",
    "    for i, x_ in enumerate(x_np):\n",
    "        for j, t_ in enumerate(t_np):\n",
    "            G_an_ftcs[i, j] = g_analytic(np.array([x_, t_]))\n",
    "    ftcs_max_abs = float(np.max(np.abs(ftcs_grid - G_an_ftcs)))\n",
    "    test_results.append({\n",
    "        'Nx': Nx,\n",
    "        'Nt': Nt,\n",
    "        'pinn_max_abs': pinn_res['max_abs'],\n",
    "        'pinn_cost': pinn_res['cost'],\n",
    "        'pinn_infer_time': pinn_res['infer_time'],\n",
    "        'ftcs_max_abs': ftcs_max_abs,\n",
    "        'ftcs_time': ftcs_time,\n",
    "    })\n",
    "    if (Nx, Nt) == target_grid:\n",
    "        plot_data = {\n",
    "            'x': x_np,\n",
    "            't': t_np,\n",
    "            'ftcs': ftcs_grid,\n",
    "            'ftcs_an': G_an_ftcs,\n",
    "            'pinn': pinn_res['g_dnn'],\n",
    "        }\n",
    "    print(f\"Nx={Nx}, Nt={Nt} | PINN max abs {pinn_res['max_abs']:.3e}, cost {pinn_res['cost']:.3e}, infer {pinn_res['infer_time']*1e3:.2f} ms | FTCS max abs {ftcs_max_abs:.3e}, time {ftcs_time*1e3:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b46e057824aaab12dc2d902724b42",
   "metadata": {},
   "source": [
    "## Heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3829ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps: FTCS vs analytical and PINN-FTCS (target grid Nx=30, Nt=30)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import numpy as np\n",
    "\n",
    "if not plot_data:\n",
    "    print(\"plot_data is empty; run the grid loop cell first.\")\n",
    "else:\n",
    "    x = plot_data['x']\n",
    "    t = plot_data['t']\n",
    "    ftcs = plot_data['ftcs']\n",
    "    ftcs_an = plot_data['ftcs_an']\n",
    "    pinn = plot_data['pinn']\n",
    "\n",
    "    def _sci_cbar(fig, im, ax):\n",
    "        fmt = ScalarFormatter(useMathText=True)\n",
    "        fmt.set_powerlimits((-2, 2))\n",
    "        cbar = fig.colorbar(im, ax=ax, shrink=0.8, format=fmt, pad=0.025)\n",
    "        off = cbar.ax.yaxis.get_offset_text()\n",
    "        off.set_position((0, 0))  # push outside plot\n",
    "        off.set_verticalalignment('bottom')\n",
    "        return cbar\n",
    "\n",
    "    # FTCS heatmap (no transpose; swap labels only)\n",
    "    fig_ftcs, ax0 = plt.subplots(figsize=(6,5))\n",
    "    im0 = ax0.imshow(ftcs, origin='lower', extent=[x.min(), x.max(), t.min(), t.max()], aspect='auto', cmap='plasma')\n",
    "    ax0.set_title('FTCS')\n",
    "    ax0.set_xlabel('t')\n",
    "    ax0.set_ylabel('x')\n",
    "    _sci_cbar(fig_ftcs, im0, ax0)\n",
    "    fig_ftcs.tight_layout()\n",
    "    save_fig('test3_ftcs_grid_30x30', fig=fig_ftcs)\n",
    "    plt.show()\n",
    "\n",
    "    # Analytical heatmap\n",
    "    fig_an, ax1 = plt.subplots(figsize=(6,5))\n",
    "    im1 = ax1.imshow(ftcs_an, origin='lower', extent=[x.min(), x.max(), t.min(), t.max()], aspect='auto', cmap='plasma')\n",
    "    ax1.set_title('Analytical')\n",
    "    ax1.set_xlabel('t')\n",
    "    ax1.set_ylabel('x')\n",
    "    _sci_cbar(fig_an, im1, ax1)\n",
    "    fig_an.tight_layout()\n",
    "    save_fig('test3_analytical_grid_30x30', fig=fig_an)\n",
    "    plt.show()\n",
    "\n",
    "    # FTCS - Analytical\n",
    "    fig_diff, ax2 = plt.subplots(figsize=(6,5))\n",
    "    diff_ftcs = ftcs - ftcs_an\n",
    "    v = float(np.max(np.abs(diff_ftcs))) or 1e-12\n",
    "    im2 = ax2.imshow(diff_ftcs, origin='lower', extent=[x.min(), x.max(), t.min(), t.max()], aspect='auto', cmap='RdBu_r', vmin=-v, vmax=v)\n",
    "    ax2.set_title('FTCS - Analytical')\n",
    "    ax2.set_xlabel('t')\n",
    "    ax2.set_ylabel('x')\n",
    "    _sci_cbar(fig_diff, im2, ax2)\n",
    "    fig_diff.tight_layout()\n",
    "    save_fig('test3_ftcs_minus_analytical_30x30', fig=fig_diff)\n",
    "    plt.show()\n",
    "\n",
    "    # PINN - FTCS\n",
    "    fig_pf, ax3 = plt.subplots(figsize=(6,5))\n",
    "    diff_pf = pinn - ftcs\n",
    "    v2 = float(np.max(np.abs(diff_pf))) or 1e-12\n",
    "    im_pf = ax3.imshow(diff_pf, origin='lower', extent=[x.min(), x.max(), t.min(), t.max()], aspect='auto', cmap='RdBu_r', vmin=-v2, vmax=v2)\n",
    "    ax3.set_title('PINN - FTCS')\n",
    "    ax3.set_xlabel('t')\n",
    "    ax3.set_ylabel('x')\n",
    "    _sci_cbar(fig_pf, im_pf, ax3)\n",
    "    fig_pf.tight_layout()\n",
    "    save_fig('test3_pinn_minus_ftcs_30x30', fig=fig_pf)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbd2e1a9cd4208bd073c3940bc6492",
   "metadata": {},
   "source": [
    "## Results table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed869dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table (DataFrame + CSV)\n",
    "import pandas as pd\n",
    "if not test_results:\n",
    "    print(\"test_results is empty; run the grid loop cell first.\")\n",
    "else:\n",
    "    df = pd.DataFrame(test_results).copy()\n",
    "    df['pinn_infer_time_ms'] = df['pinn_infer_time'] * 1e3\n",
    "    df['ftcs_time_ms'] = df['ftcs_time'] * 1e3\n",
    "    cols = ['Nx', 'Nt', 'pinn_max_abs', 'pinn_cost', 'pinn_infer_time_ms', 'ftcs_max_abs', 'ftcs_time_ms']\n",
    "    df = df[cols]\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331e1ce12864b28a6750c9d03e84348",
   "metadata": {},
   "source": [
    "## Summary plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca22b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max error and runtime comparisons vs grid resolution.\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "if not test_results:\n",
    "    print(\"test_results is empty; run the grid loop cell first.\")\n",
    "else:\n",
    "    Nx_vals = [r['Nx'] for r in test_results]\n",
    "    ftcs_mad = [r['ftcs_max_abs'] for r in test_results]\n",
    "    pinn_mad = [r['pinn_max_abs'] for r in test_results]\n",
    "    ftcs_time = [r['ftcs_time'] for r in test_results]\n",
    "    pinn_time = [r['pinn_infer_time'] for r in test_results]\n",
    "\n",
    "    try:\n",
    "        x_vals = np.array(Nx_vals, dtype=float)\n",
    "        xticks = None\n",
    "    except Exception:\n",
    "        x_vals = np.arange(len(Nx_vals))\n",
    "        xticks = Nx_vals\n",
    "\n",
    "    order = np.argsort(x_vals)\n",
    "    x_sorted = x_vals[order]\n",
    "    ftcs_sorted = np.array(ftcs_mad)[order]\n",
    "    pinn_sorted = np.array(pinn_mad)[order]\n",
    "    ftcs_time_sorted = np.array(ftcs_time)[order]\n",
    "    pinn_time_sorted = np.array(pinn_time)[order]\n",
    "\n",
    "    fig, ax_left = plt.subplots(figsize=(10,6))\n",
    "    ax_right = ax_left.twinx()\n",
    "\n",
    "    h1 = ax_left.plot(x_sorted, ftcs_sorted, color='tab:blue', marker='o', linestyle='-', label='FTCS')[0]\n",
    "    h2 = ax_right.plot(x_sorted, pinn_sorted, color='tab:red', marker='s', linestyle='-', label='PINN')[0]\n",
    "\n",
    "    for xi, fval, t in zip(x_sorted, ftcs_sorted, ftcs_time_sorted):\n",
    "        ax_left.annotate(f'{t*1e3:.1f} ms', (xi, fval), textcoords='offset points', xytext=(0,-10), ha='center', fontsize=12, color='tab:blue')\n",
    "    for xi, pval, t in zip(x_sorted, pinn_sorted, pinn_time_sorted):\n",
    "        ax_right.annotate(f'{t*1e3:.1f} ms', (xi, pval), textcoords='offset points', xytext=(0,8), ha='center', fontsize=12, color='tab:red')\n",
    "\n",
    "    ax_left.set_xlabel('Nx grid')\n",
    "    ax_left.set_ylabel('FTCS max abs(error)', color='tab:blue')\n",
    "    ax_right.set_ylabel('PINN max abs(error)', color='tab:red')\n",
    "    ax_left.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax_right.tick_params(axis='y', labelcolor='tab:red')\n",
    "    if xticks is not None:\n",
    "        ax_left.set_xticks(x_sorted)\n",
    "        ax_left.set_xticklabels(xticks)\n",
    "\n",
    "    by_label = OrderedDict()\n",
    "    by_label[h1.get_label()] = h1\n",
    "    by_label[h2.get_label()] = h2\n",
    "    ax_left.legend(by_label.values(), by_label.keys(), loc='center right')\n",
    "\n",
    "    plt.title(f'Final MAE vs Grid Resolution â€” {architecture}, activation {activation_label}')\n",
    "    plt.tight_layout()\n",
    "    save_fig('test3_mad_vs_grid')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1a2b96c734037b8b8e047cbfcae21",
   "metadata": {},
   "source": [
    "## Timing summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing + cost summary\n",
    "print('=== Training ===')\n",
    "print(f'PINN training: {train_time:.3f} s | final cost {train_final_cost:.4e} (arch {architecture}, activation {activation_label})')\n",
    "\n",
    "print('=== Per-grid inference vs FTCS ===')\n",
    "for r in test_results:\n",
    "    print(f\"Nx={r['Nx']:>3}, Nt={r['Nt']:>3} | PINN infer {r['pinn_infer_time']*1e3:.2f} ms, max|err| {r['pinn_max_abs']:.3e}, cost {r['pinn_cost']:.3e} | FTCS time {r['ftcs_time']*1e3:.2f} ms, max|err| {r['ftcs_max_abs']:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f53b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
