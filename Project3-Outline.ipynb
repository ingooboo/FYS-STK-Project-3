{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc9ed7c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html Project3.do.txt  -->\n",
    "<!-- dom:TITLE: Project 3 on Machine Learning, deadline December 15 (midnight), 2025 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1f357",
   "metadata": {},
   "source": [
    "# Solving partial differential equations with neural networks\n",
    "## Project 3 - FYS-STK4155\n",
    "\n",
    "Deadline: **December 15, 2025**\n",
    "\n",
    "Authors: **Ingvild Olden and Jenny Guldvog**\n",
    "\n",
    "### Project information\n",
    "\n",
    "This is a project on using **machine learning methods** (neural networks mainly) to the **solution of ordinary differential equations and partial differential equations**, with a final twist on **how to diagonalize a symmetric matrix with neural networks**. We will study the **solution of the diffusion equation in one dimension** using a standard explicit scheme and neural networks to solve the same equations. This is a field with large scientific interest, spanning from studies of turbulence in fluid mechanics and meteorology to the solution of quantum mechanical systems. As reading background you can use the slides [from week 43](https://compphysics.github.io/MachineLearning/doc/pub/week43/html/week42.html)\n",
    "and/or the textbook by [Yadav et al](https://www.springer.com/gp/book/9789401798150).\n",
    "\n",
    "### Project outline\n",
    "\n",
    "1. **Setting up the problem, the algirithms and the equations needed to be implementes, the analytical solution to the problem.**\n",
    "2. **Implementation of the explicit scheme algorithm and tests of different solutions (dictated by the stability limit), a study of the solutions at two time points, with one smooth curved, and one almost linear $u$.**\n",
    "3. **Implementation of either own code, or functionalities from tensorflow/keras to solve the same equation, and a comparison to the analytical solution.**\n",
    "4. **A study of the stability of the results, by tuning the number of hidden nodes, layers and activation functions.**\n",
    "5. **A critical assessment and discussion of the methods and the potential for the solving differential equations with machine learning methods.**\n",
    "\n",
    "#### Notes:\n",
    "- How to or if to scale the data?!\n",
    "- Use litterature and sources to actually discuss the results more, not just state them. \n",
    "- Figures should be clear in size and texts!\n",
    "- Motivate the problem in the introduction.\n",
    "- Give a conclusion, and an outlook (future work)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c2d8c",
   "metadata": {},
   "source": [
    "## Part a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1dec1",
   "metadata": {},
   "source": [
    "### Setting up the problem\n",
    "\n",
    "The first part deals with structuring and reading the data, much along the same lines as done in projects 1 and 2.  Explain how the data are produced and place them in a proper context. **The physical problem can be that of the temperature gradient in a rod** of length $L=1$ at $x=0$ and $x=1$.\n",
    "We are looking at a one-dimensional problem \n",
    "$$\n",
    "\\frac{\\partial^2 u(x,t)}{\\partial x^2} =\\frac{\\partial u(x,t)}{\\partial t}, t> 0, x\\in [0,L]\n",
    "$$\n",
    "or\n",
    "$$\n",
    "u_{xx} = u_t,\n",
    "$$\n",
    "with initial conditions, i.e., the conditions at $t=0$,\n",
    "$$\n",
    "u(x,0)= \\sin{(\\pi x)} \\hspace{0.5cm} 0 < x < L,\n",
    "$$\n",
    "with $L=1$ the length of the $x$-region of interest. The boundary conditions are\n",
    "$$\n",
    "u(0,t)= 0 \\hspace{0.5cm} t \\ge 0,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "u(L,t)= 0 \\hspace{0.5cm} t \\ge 0.\n",
    "$$\n",
    "The function $u(x,t)$ can be the temperature gradient of a rod. As time increases, the velocity approaches a linear variation with $x$. **We will limit ourselves to the so-called explicit forward Euler algorithm with discretized versions of time given by a forward formula and a centered difference in space** resulting in\n",
    "$$\n",
    "u_t\\approx \\frac{u(x,t+\\Delta t)-u(x,t)}{\\Delta t}=\\frac{u(x_i,t_j+\\Delta t)-u(x_i,t_j)}{\\Delta t}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "u_{xx}\\approx \\frac{u(x+\\Delta x,t)-2u(x,t)+u(x-\\Delta x,t)}{\\Delta x^2},\n",
    "$$\n",
    "or\n",
    "$$\n",
    "u_{xx}\\approx \\frac{u(x_i+\\Delta x,t_j)-2u(x_i,t_j)+u(x_i-\\Delta x,t_j)}{\\Delta x^2}.\n",
    "$$\n",
    "**Write down the algorithm and the equations you need to implement. Find also the analytical solution to the problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81529f",
   "metadata": {},
   "source": [
    "## Part b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f159ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Implementation \n",
    "\n",
    "You need to include at least two central algorithms, or as an alternative explore methods from decisions tree to bagging, random forests and boosting. Explain the basics of the methods you have chosen to work with. This would be your theory part. Implement the explicit scheme algorithm and perform tests of the solution for $\\Delta x=1/10$, $\\Delta x=1/100$ using $\\Delta t$ as dictated by the stability limit of the explicit scheme. The stability criterion for the explicit scheme requires that $\\Delta t/\\Delta x^2 \\leq 1/2$. Study the solutions at two time points $t_1$ and $t_2$ where $u(x,t_1)$ is smooth but still significantly curved and $u(x,t_2)$ is almost linear, close to the stationary state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a795b9b",
   "metadata": {},
   "source": [
    "## Part c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0669495",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Neural networks\n",
    "\n",
    "Then describe your algorithm and its implementation and tests you have performed. Study now the lecture notes on solving ODEs and PDEs with neural\n",
    "network and use either your own code from project 2 or the functionality of tensorflow/keras to solve the same equation as in part b).  Discuss your results and compare them with the standard explicit scheme. Include also the analytical solution and compare with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6028624",
   "metadata": {},
   "source": [
    "## Part d) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3dd3ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Neural network complexity\n",
    "\n",
    "Then presents your results and findings, link with existing literature and more. Here we study the stability of the results of the results as functions of the number of hidden nodes, layers and activation functions for the hidden layers. Increase the number of hidden nodes and layers in order to see if this improves your results. Try also different activation functions for the hidden layers, such as the **tanh**, **ReLU**, and other activation functions. Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03777ec9",
   "metadata": {},
   "source": [
    "## Part e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68f43c",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dscussion\n",
    "\n",
    "Here you should present a critical assessment of the methods you have studied and link your results with the existing literature. Finally, present a critical assessment of the methods you have studied and discuss the potential for the solving differential equations with machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d369fe5",
   "metadata": {},
   "source": [
    "## Physics-Informed Neural Network - PINN - with FFNN (Physical neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f76be",
   "metadata": {},
   "source": [
    "### From lecture: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d03b43",
   "metadata": {},
   "source": [
    "When you solve a differential equation with a neural network you are going to start with a guess on what the function looks like. So your solution is going to contain two parts (often). One part which contains your boundary and initial conditions, and another part which is your neural network. So it is a neural network which is going to find the best representation of the solution. And the way you do that, is that youre going to take the difference between the left hand side and the right hand side of a differential equation. That difference is something you want to drive to zero (minimize). You square that difference, so it is going to look like a mean squared error. And then for every iteration with an trial or a 'guess' for what the solution is, youre driving this difference closer and closer to zero. This is the esssence of solving differenctial equations. However, tailoring the boundary conditions and the initial conditions so that the network can actually work properly is an art by itself. Tailoring properly the initial conditions and the boundary condiitons is a very important aspect of getting a stable solution to your differential equations. \n",
    "\n",
    "Implementation: \n",
    "1. Set up a trial function (anzats?) that obays the initial conditions (boundary conditions) (This is where the physics is implemented/informed)\n",
    "2. Bake this in a new function by a NN\n",
    "3. Cost function: Difference between LHS and RHS squared\n",
    "4. Find an activation function closer to thw problem (e.g. Gaussian problem --> GLO)\n",
    "5. Flexibility with automatic differentiation, instead of symbolic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ff4d2",
   "metadata": {},
   "source": [
    "### Matlab text:\n",
    "\n",
    "https://se.mathworks.com/discovery/physics-informed-neural-networks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549389e",
   "metadata": {},
   "source": [
    "**Very good outline and introduction to the theme, not good for finding sources!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaad236",
   "metadata": {},
   "source": [
    "PINN are Neural networks that incorporate physical laws descibed by differential equations into their loss functions to guide the learning process toward solutions that are more consisten with the underlying physics. They are used to approximate solutions to partial differential equations (PDEs) and ordinary differential equations (ODEs). Unlike purely data-driven approaches, which learn mathematical relationships solely from input and output data, PINNs use prior physics knowledge, and make more accurate predictions outside of the training data, and are more effective with limited or noisy training data. \n",
    "\n",
    "Unlike traditional numerical methods for solving differential equations, such as finite element analysis for PDEs, PINNs are mesh free, an can approximate high-dimensional PDE solutions. They can also solve for missing model parameters such as unknown PDE or ODE coefficients. And they can solve ill-posed problems where no boundary data exists. Thay also easily incorporate sparse or noisy measurements. \n",
    "\n",
    "While PINNs offer potential benefits compared with purely data-driven methods and traditional numerical methods, PINNs do have some limitations and challanges. This include limited convergence theory and lack of unified traning strategies. Additionally computational cost of calculating high-order derivatives, and difficulty learning high-frequency and multiscale components of PDE solutions. \n",
    "\n",
    "However, PINNs are a dynamically research area, with ongoing advancements to address and overcome these current challenges and limitations. \n",
    "\n",
    "#### How PINNs differ from traditional neural networks\n",
    "\n",
    "PINNs differ from traditional neural networks in their ability to incorporate a priori domain knowledge of the problem in the form of differential equations. This additional information enables PINNs to make more accurate predictions outside of the given measurement data. Furthermore, the additional physics knowledge regularizes the predicted solution in the presence of noisy measurement data, enabling PINNs to learn the true underlying signal rather than overfitting the noisy data.\n",
    "\n",
    "For example, consider a scenario where the noisy measurements, $\\theta_{meas}$, of a system of interest have been collected, and the objective is to predict future values of the system, $\\theta{pred}$, with a feedforward artificial neural network. The network is trained using the available measurements and will be used to predict unseen future values. Training a regression neural network typically involves minimizing the mean-squared error between the neural network’s predictions and the provided measurements.\n",
    "\n",
    "The neural network struggles to accurately predict values of the system outside of the training data. Acquiring more data could enhance predictions, yet this approach may be prohibitively expensive or impossible for many applications. However, often the domain expert possesses deeper knowledge about the underlying physical process that governs the system they are studying. Specifically, in this scenario, the measurements represent the angle of displacement from vertical of a payload swinging from a crane. This process can be simplistically represented by a damped pendulum, which can be approximately modeled for small angles by a linear, second-order differential equation:\n",
    "$$\n",
    "\\theta''(t) + 2\\beta\\theta'(t)+\\omega_0^2\\theta(t) = 0\n",
    "$$\n",
    "Rather than ignoring this knowledge, PINNs incorporate the differential equation as an additional, physics-informed term in the loss function. PINNs evaluate the residual of the differential equation at additional points in the domain, which provides more information to the PINN without the need for more measurements. While this toy example can be solved analytically, it illustrates the concepts behind PINNs. **During training, PINNs find a balance between fitting the given measurements and the underlying physical process.** By incorporating an extra physics loss term, PINNs can outperform traditional neural networks in making predictions in the presence of noisy measurements and in data regimes without measurements.\n",
    "\n",
    "#### How PINNs work\n",
    "\n",
    "PINNs use optimization algorithms to iteratively update the parameters of a neural network until the value of a specified, physics-informed loss function is decreased to an acceptable level, pushing the network toward a solution of the differential equation.\n",
    "\n",
    "PINNs have loss functions, $L$, which consist of several terms: the physics-informed loss term, $L_{Physics}$, and optionally terms that evaluate the error between the values predicted by the network and any values prescribed by initial and/or boundary data, $L_{Conds}$, and other additional measurements, $L_{Data}$. The physics-informed loss term evaluates the residual of the differential equation at points in the domain using automatic differentiation (AD) or other numerical differentiation techniques. Because the physics-informed term does not compute the error between a prediction and a target value, this term can be considered an unsupervised loss term, meaning that the network can be trained with any points from the domain, even without measurements at these points.\n",
    "\n",
    "First introduced in 2017, PINNs now have many variations, including:\n",
    "\n",
    "- Bayesian PINNs (BPINNs), which use the Bayesian framework to allow for uncertainty quantification\n",
    "- Variational PINNs (VPINNs), which incorporate the weak form of a PDE into the loss function\n",
    "- First-order formulated PINNs (FO-PINNs), which can be faster and more accurate for solving higher-order PDEs than standard PINNs\n",
    "\n",
    "In addition, PINNs can be used with different neural network architectures, such as graph neural networks (GNNs), Fourier neural operators (FNOs), deep operator networks (DeepONets), and others, yielding so-called physics-informed versions of these architectures.\n",
    "\n",
    "#### PINN Applications\n",
    "\n",
    "PINNs leverage the power of deep learning while improving compliance with physical laws, making them a versatile tool for applications where physics is fully or partially known, such as the case of a PDE or ODE with unknown coefficients. Applications of PINNs include:\n",
    "\n",
    "- **Heat transfer**, specifically for modeling heat distribution and transfer processes. PINNs can embed the governing equations that model thermal processes in materials and systems, such as the heat equation, into the loss function. This approach ensures that the solutions adhere to these physical laws, leading to physically plausible predictions. In addition, PINNs can replace expensive numerical simulations to quickly approximate temperature distributions over parameterized geometries in design optimization applications. Furthermore, PINNs can be used in inverse problems to identify unknown material properties, such as thermal conductivity.\n",
    "- **Computational fluid dynamics (CFD)**, specifically for approximating velocity, pressure, and temperature fields of fluids by incorporating the Navier-Stokes equations in the loss function. PINNs can be used in mesh-free forward simulations to accurately predict these quantities or in inverse problems where the goal is to infer unknown parameters or inputs, such as boundary conditions, source terms, or fluid properties, from observed data.\n",
    "- **Structural mechanics**, for solving both forward and inverse problems by embedding the governing physical laws, such as equations of elasticity and structural dynamics, directly into the loss function. This integration enables PINNs to accurately predict structural responses such as deformations, stresses, and strains under various loads and conditions, as well as identify unknown material properties or external loads based on observed data. Particularly useful in scenarios where traditional analytical solutions are infeasible or data is scarce, PINNs reduce the dependency on extensive data sets by leveraging physical principles to guide the learning process. The flexibility of PINNs enables them to handle complex problems, including nonlinear material behavior and multiphysics modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e3f0e",
   "metadata": {},
   "source": [
    "### Wikipedia : \n",
    "\n",
    "https://en.wikipedia.org/wiki/Physics-informed_neural_networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e0e85",
   "metadata": {},
   "source": [
    "**Good introduction and finding other sources!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba169572",
   "metadata": {},
   "source": [
    "Physics-informed neural networks (PINNs), also referred to as Theory-Trained Neural Networks (TTNs), are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs). Low data availability for some biological and engineering problems limit the robustness of conventional machine learning models used for these applications. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the generalizability of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. For they process continuous spatial and time coordinates and output continuous PDE solutions, they can be categorized as neural fields.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3d467",
   "metadata": {},
   "source": [
    "### Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations - Maziar Raissi, Paris Perdikaris, George Em Karniadakis\n",
    "\n",
    "https://arxiv.org/abs/1711.10561"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa08e3",
   "metadata": {},
   "source": [
    "**This might be the 'first' paper to introduce?? Good for other sources!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b01c9",
   "metadata": {},
   "source": [
    "We introduce physics informed neural networks - neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-effcient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtainphysics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2ada6",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a7200",
   "metadata": {},
   "source": [
    "- Adaptive activation functions accelerate convergence in deep and physics-informed neural networks: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0021999119308411?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- When and why PINNs fail to train: A neural tangent kernel perspective: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S002199912100663X?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0045782520302127?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0045782521000773?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Physics-Informed Neural Networks for Heat Transfer Problems: https://asmedigitalcollection-asme-org.ezproxy.uio.no/heattransfer/article/143/6/060801/1104439/Physics-Informed-Neural-Networks-for-Heat-Transfer\n",
    "\n",
    "- NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0021999120307257?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Physics-informed neural networks for high-speed flows: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0045782519306814?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Understanding and Mitigating Gradient Flow Pathologies in Physics-Informed Neural Networks: https://epubs-siam-org.ezproxy.uio.no/doi/10.1137/20M1318043\n",
    "\n",
    "- Physics-informed neural networks (PINNs) for fluid mechanics: a review: https://link-springer-com.ezproxy.uio.no/article/10.1007/s10409-021-01148-1?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot&getft_integrator=clarivate\n",
    "\n",
    "- Reynolds averaged turbulence modelling using deep neural networks with embedded invariance: https://www-cambridge-org.ezproxy.uio.no/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB\n",
    "\n",
    "- Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next: https://link-springer-com.ezproxy.uio.no/article/10.1007/s10915-022-01939-z?utm_source=getftr&utm_medium=getftr&utm_campaign=getftr_pilot&getft_integrator=clarivate\n",
    "\n",
    "- DeepXDE: A Deep Learning Library for Solving Differential Equations: https://epubs-siam-org.ezproxy.uio.no/doi/10.1137/19M1274067\n",
    "\n",
    "- GDOSphere: A spherical graph neural network framework with neural operators for weather forecasting: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0378437125004248?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- A Machine Learning Tutorial for Operational Meteorology. Part II: Neural Networks and Deep Learning: https://journals-ametsoc-org.ezproxy.uio.no/view/journals/wefo/38/8/WAF-D-22-0187.1.xml?utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Physics-informed neural networks to reconstruct surface velocity field from drifter data: https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2025.1547995/full\n",
    "\n",
    "- Deep learning-based solution for the KdV-family governing equations of ocean internal waves: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S1463500324001793?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0021999118307125?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- An energy approach to the solution of partial differential equations in computational mechanics via machine learning: Concepts, implementation and applications: https://www-sciencedirect-com.ezproxy.uio.no/science/article/pii/S0045782519306826?pes=vor&utm_source=clarivate&getft_integrator=clarivate\n",
    "\n",
    "- Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations: https://www-science-org.ezproxy.uio.no/doi/full/10.1126/science.aaw4741?getft_integrator=clarivate&utm_source=clarivate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
